# 数据分析

1.  user_id_num 扩大后，精度得到提高，因为加入用户数后会增加数据的信息量
2.  creative_id_num 扩大后，精度可能下降
    -   因为加入大量 TF-IDF 过低的素材，不能提供辨识作用，还会增加噪声；
    -   因为加入大量 sparsity 过高的素材，使得数据过于稀疏，不方便分类；
3.  creative_id_num 缩小后，精度也会下降，而且系统过拟合的时间会提前
4.  二次训练可以提高测试集的精度，但是如果噪声过多时，二次训练可能效果就不好了
5.  max_len 必须与训练数据的词典容量匹配，过小就没法将足够的信息提供给模型，过大也会因为引入过多噪声，导致精度下降
6.  多分类问题，creative_id_num 必须能够提供足够的信息，并且需要与 max_len 匹配
7.  当参数超过 100,000,000 后，GPU 就会转成稀疏计算，计算速度会变慢 20%
   -   例如：creative_id_num=781240, embedding_size=128, 总的参数个数=100,000,010，这个时候还没有转成稀疏矩阵
8.  使用 `GlobalMaxPooling1D+(Dropout+Dense+BatchNormalization+Activation)*n`，可以得到比较好的多分类效果
    -   随着 n 的增大，分类偏向性越明显，而分类精确度下降是因为数据特征本身不具有好的可分性。学习得到的分类偏向性越明确，就越可能会误分类。
    -   特别是数据不平衡时，例如：类别`2`的数据量比较大，那么就会学得`2`的分布，对于像`2`的数据，就会给出比较高的概率，于是就容易分给`2`这个类别，但是本身数据缺少足够的分类特征，因此某些数据是无法给出这么高的分类概率的。
    -   因此`n`越高时，过拟合会越严重，解决办法就是增加分类的特征，增强数据的可分性，即减少数据的类内方差。
9.  缩小 `embedding_size` 会导致数据特征提供的信息不足，和上面的数据特征可分性不足的问题不一样，是信息量不足，导致数据不容易分离，即数据的类间方差过小
    -   小的 `embedding_size` 会使数据不平衡问题越发严重。考虑 K 均值问题，如果大量数据聚焦在一起，那么大部分数据都会分配给数量大的类别，而数量小的类别可能分配到的数据为零。
10.  使用 `LSTM+(Dropout+Dense+BatchNormalization+Activation)*n`效果较差，需要考虑没有充分提供时间序列信息的数据是否可以使用循环神经网络来提取特征？